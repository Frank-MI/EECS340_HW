\relax 
\select@language{english}
\@writefile{toc}{\select@language{english}}
\@writefile{lof}{\select@language{english}}
\@writefile{lot}{\select@language{english}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Warm-up: Big-Oh and Counting Primitive Operations}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Solve R-1.20, R-1.22, and R-1.23 in the text}{1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.1}R-1.20}{1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.2}R-1.22}{1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.3}R-1.23}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Intuitively, $2^x\in O(3^x)$, since $3^x$ grows faster. Is $3^x\in O(2^x)$?}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Intuitively, $\qopname  \relax o{log}_3(x)\in O(\qopname  \relax o{log}_2(x))$. Is $\qopname  \relax o{log}_2(x)\in O(\qopname  \relax o{log}_3(x))$?}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4}Use summations to derive tight asymptotic bounds $(\Theta (-))$ on the runtime of each algorithm}{2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.4.1}R-1.12}{2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.4.2}R-1.14}{2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.4.3}R-1.15}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5}Explain why it is reasonable to ignore the overhead of a ranged \emph  {for} loop when you derived the tight asymptotic runtime bounds in the previous question.}{3}}
\@writefile{toc}{\contentsline {section}{\numberline {2}A Challenging Sum}{3}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Theory: Big-Oh and Derivatives}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Is $f(x)\in O(g(x))$ if and only if $f'(x)\in O(g'(x))$?}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Is it true that if $\qopname  \relax m{lim}_{x\to +\infty } f'(x)=0$, then $f(x)\in O(1)$?}{4}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Theory: Properties of Big-Oh Notation}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Let $f,g: \mathbb  {R} \to \mathbb  {R}$ be continuous and strictly increasing. Is it necessarily the case that $f \in O(g(x))$ or $g \in O(f(x))$? Prove, or provide a counterexample.}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}R-1.18}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}R-1.19}{5}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Application: Matrix Multiplication}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Provide a tight asymptotic upper bound on the runtime of easy-mutiply in terms of $n$.}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}What is the asymptotic runtime of combined-multiply in terms of $n$?}{6}}
\@writefile{toc}{\contentsline {section}{\numberline {6}``Application'': Spaghetti Sort}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Describe the asymptotic runtime of unmodified spaghetti-sort.}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Suppose that we require that the input list is \emph  {not} a list of arbitrary natural numbers, but is instead a list of 64-bit unsigned integers. What is a tight bound on the asymptotic runtime of spaghetti sort now?}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}In the light of the answer to the previous question, why/ why not would we want to use spaghetti-sort for sorting 64-bit unsigned integers in practice?}{6}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Algorithm Design: Finding Cycles}{6}}
