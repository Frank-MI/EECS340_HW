\relax 
\select@language{english}
\@writefile{toc}{\select@language{english}}
\@writefile{lof}{\select@language{english}}
\@writefile{lot}{\select@language{english}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Warm-up: Big-Oh and Counting Primitive Operations}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Solve R-1.20, R-1.22, and R-1.23 in the text}{1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.1}R-1.20}{1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.2}R-1.22}{1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.3}R-1.23}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Intuitively, $2^x\in O(3^x)$, since $3^x$ grows faster. Is $3^x\in O(2^x)$?}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Intuitively, $\qopname  \relax o{log}_3(x)\in O(\qopname  \relax o{log}_2(x))$. Is $\qopname  \relax o{log}_2(x)\in O(\qopname  \relax o{log}_3(x))$?}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4}Use summations to derive tight asymptotic bounds $(\Theta (-))$ on the runtime of each algorithm}{2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.4.1}R-1.12}{2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.4.2}R-1.14}{2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.4.3}R-1.15}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5}Explain why it is reasonable to ignore the overhead of a ranged \emph  {for} loop when you derived the tight asymptotic runtime bounds in the previous question.}{3}}
\@writefile{toc}{\contentsline {section}{\numberline {2}A Challenging Sum}{3}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Theory: Big-Oh and Derivatives}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}}{4}}
