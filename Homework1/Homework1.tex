\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{graphicx}
\usepackage{array}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\renewcommand{\baselinestretch}{1.0}
\usepackage[letterpaper, margin=0.75in]{geometry}
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}

\begin{document}
	\title{Homework1 for EECS 340}
	\author{Yu Mi}
	\maketitle
\section{Warm-up: Big-Oh and Counting Primitive Operations}
\emph{Show your work} on the following questions. Use the limit-based definitions of asymptotic notation on the ``Big-Oh Cheat Sheet'' on Canvas wherever applicable.
\subsection{Solve R-1.20, R-1.22, and R-1.23 in the text}
\subsubsection{R-1.20}
	Show that $(n+1)^5$ is $O(n^5)$.

	\emph{Proof:} Let $f(n)=(n+1)^5$ and $g(n)=n^5$ so that
	\begin{equation*}
	\begin{multlined}
		\lim_{n\to\infty} \frac{f(n)}{g(n)}=\lim_{n\to\infty} \frac{(n+1)^5}{n^5}=\lim_{n\to\infty} \frac{n^5+5n^4+10n^3+10n^2+5n+1}{n^5}\\
		=1+\lim_{n\to\infty}\frac{5}{n}+\frac{10}{n^2}+\frac{10}{n^3}+\frac{5}{n^4}+\frac{1}{n^5}=1
	\end{multlined}
	\end{equation*}

	Since $0\leq1<\infty$, $(n+1)^5$ is $O(n^5)$.
\subsubsection{R-1.22}
	Show that $n$ is $o(n\log n)$.
	
	\emph{Proof:} Let $f(n)=n$ and $g(n)=n\log n$ so that
	\begin{equation*}
		\lim_{n\to\infty} \frac{f(n)}{g(n)}=\lim_{n\to\infty} \frac{n}{n\log n}=\lim_{n\to\infty} \frac{1}{\log n}=0
	\end{equation*}
	
	Since $0=0$, $n$ is $o(n\log n)$.
\subsubsection{R-1.23}
	Show that $n^2$ is $\omega(n)$.
	
	\emph{Proof:} Let $f(n)=n^2$ and $g(n)=n$ so that
	\begin{equation*}
		\lim_{n\to\infty} \frac{f(n)}{g(n)}=\lim_{n\to\infty} \frac{n^2}{n}=\lim_{n\to\infty} n=\infty
	\end{equation*}
	
	Since $\infty = \infty$, $n^2$ is $\omega(n)$.
\subsection{Intuitively, $2^x\in O(3^x)$, since $3^x$ grows faster. Is $3^x\in O(2^x)$?}
	\emph{Answer:} No, proof as follows:
	
	Let $f(x)=3^x$ and $g(x)=2^x$ so that
	\begin{equation*}
		\lim_{x\to\infty} \frac{f(x)}{g(x)}=\lim_{x\to\infty} \frac{3^x}{2^x}=\lim_{x\to\infty} (\frac{3}{2})^x=\infty
	\end{equation*}
	
	Since $\infty = \infty$, so that $3^x$ is $\omega(2^x)$, $3^x\not\in O(2^x)$
\subsection{Intuitively, $\log_3(x)\in O(\log_2(x))$. Is $\log_2(x)\in O(\log_3(x))$?}
	\emph{Answer:} Yes, proof as follows:
	
	Let $f(x)=\log_2(x)$ and $g(x)=\log_3(x)$ so that
	\begin{equation*}
		\lim_{x\to\infty} \frac{\log_2(x)}{\log_3(x)} = \lim_{x\to\infty} \frac{\frac{\ln x}{\ln 2}}{\frac{\ln x}{\ln 3}} = \lim_{x\to\infty} \frac{\ln 3}{\ln 2} = \log_2(3)
	\end{equation*}
	
	Since $0\leq \log_2(3)<\infty$, $\log_2(x)$ is $O(\log_3(x))$.
\subsection{Use summations to derive tight asymptotic bounds $(\Theta (-))$ on the runtime of each algorithm}
\subsubsection{R-1.12}
	\emph{Answer:}
	First, we need to rewrite this algorithm into \emph{while} loop:
	
	\textbf{Algorithm} Loop2($n$):
	
	\begin{algorithmic}
		\State $p \gets 1$ \Comment $1$ unit of time
		\State $i \gets 1$ \Comment $1$ unit of time
		\While {$i \leq 2n$} \Comment $2n+1$ units of time
			\State $p \gets p \times i$ \Comment $2\times 2n$ units of time
			\State $i \gets i + 1$ \Comment $2\times 2n$ units of time
		\EndWhile
	\end{algorithmic}
	
	As is described in the comments of the algorithm, the run time of this algorithm should be $\Theta(10n+3)$ units of time. So that this algorithm is $\Theta(n)$
\subsubsection{R-1.14}
	\emph{Answer:}
	First, we need to rewrite this algorithm into \emph{while} loop:
	
	\textbf{Algorithm} Loop4($n$):
	
	\begin{algorithmic}
		\State $s \gets 0$ \Comment $1$ unit of time
		\State $i \gets 1$ \Comment $1$ unit of time
		\While {$i \leq 2n$} \Comment $2n+1$ units of time
			\State $j \gets 1$ \Comment $2n$ units of time
				\While {$j \leq i$} \Comment $(i+1) \times 2n$ units of time
					\State $s \gets s+i$ \Comment $2\times i \times 2n$ units of time
					\State $j \gets j+1$ \Comment $2\times i \times 2n$ units of time
				\EndWhile
			\State $i \gets i+1$ \Comment $2n$ units of time
		\EndWhile
	\end{algorithmic}
	
	To calculate the time cost of the inner loop, we need to focus on the value of $i$ which changes with the outer loop. To make calculate easy to understand, we define $C_1$ as the actual units of time the outer loop will cost and $C_2$ as the actual units of time the inner loop will cost. So that:
	\begin{equation*}
		C_1=1+1+2n+1+2n+2n = 6n+3 
	\end{equation*}
	\begin{equation*}
		C_2=2n\sum_{i=1}^{2n} (i+1+2i+2i)=10n^2+7n
	\end{equation*}
	To sum up, the total units of time this algorithm will cost should be $time_{total} = C_1 + C_2 = 10n^2+13n+3$. So that this algorithm is $\Theta(n^2)$
\subsubsection{R-1.15}
	\emph{Answer:}
	First, we need to rewrite this algorithm into \emph{while} loop:
	
	\textbf{Algorithm} Loop5($n$):
	
	\begin{algorithmic}
		\State $s\gets0$ \Comment $1$ unit of time
		\State $i\gets1$ \Comment $1$ unit of time
		\While{$i \leq n^2$} \Comment $n^2+1$ units of time
			\State $j \gets 1$ \Comment $n^2$ units of time
			\While{$j \leq i$} \Comment $(i+1) \times n^2$ units of time
				\State $s\gets s+i$ \Comment $2\times i \times n^2$ units of time
				\State $j\gets j+1$ \Comment $2\times i \times n^2$ units of time
			\EndWhile
			\State $i\gets i+1$ \Comment $n^2$ units of time
		\EndWhile
	\end{algorithmic}
	
	To calculate the time cost of the inner loop, we need to focus on the value of $i$ which changes with the outer loop. To make calculate easy to understand, we define $C_1$ as the actual units of time the outer loop will cost and $C_2$ as the actual units of time the inner loop will cost. So that:
	\begin{equation*}
		C_1=1+1+n^2+1+n^2+n^2 = 3n^2+3
	\end{equation*}
	\begin{equation*}
		C_2=2n\sum_{i=1}^{n^2} (i+1+2i+2i)=\frac{5}{2} n^4 + \frac{7}{2} n^2
	\end{equation*}
	
	To sum up, the total units of time this algorithm will cost should be $time_{total} = C_1 + C_2 = \frac{5}{2}n^4+\frac{13}{2}n^2+3$. So that this algorithm is $\Theta(n^4)$
\subsection{Explain why it is reasonable to ignore the overhead of a ranged \emph{for} loop when you derived the tight asymptotic runtime bounds in the previous question.}
	\emph{Answer:} When calculating the runtime bounds of a loop, the overhead of such loop will cost a constant time of computation, while the loop body will cost $n$ times more than the overhead. As $n$ grows big enough, the overhead is always significantly smaller than the loop body. Thus, when we are deriving the asymptotic runtime bounds, we can ignore the overhead of a ranged loop.
\section{A Challenging Sum}
	Show that summation $\sum_{i=1}^{n} \ceil{\log_2(n/i)}$ is $O(n)$. You may assume that $n$ is a power of $2$.
	
	\emph{Proof:}
	
	\textbf{Base Case:} Show that the statements holds for $n=1$.
	
	When $n=1$, $\sum_{i=1}^{n} \ceil{\log_2(n/i)}=\ceil{\log_2(1)}=0$, and $n=1$, so we have $0\leq1$.
	
	\textbf{Inductive Step:} Show that if $\sum_{i=1}^{n} \ceil{\log_2(n/i)}\leq cn$ holds, then also $\sum_{i=1}^{2n} \ceil{\log_2(2n/i)}\leq c2n$ holds.
	
	Since we have $\sum_{i=1}^{n} \ceil{\log_2(n/i)}\leq cn$ holds, we can replace $n$ by $2n$, thus we have:
	\begin{equation*}
		\sum_{i=1}^{2n} \ceil{\log_2(2n/i)} = \sum_{i=1}^{2n} \ceil{\log_2(n/i)+1}=2n+\sum_{i=1}^{2n}\ceil{\log_2(n/i)}
	\end{equation*}
	
	When $n<i\leq2n$, $0.5\leq(n/i)<1$. Thus $-1\leq\log_2(n/i)<0$, thus we have
	\begin{equation*}
		2n+\sum_{i=1}^{2n}\ceil{\log_2(n/i)} = 2n-1+\sum_{i=1}^{n}\ceil{\log_2(n/i)}\leq cn+2n-1
	\end{equation*}
	
	For any constant $c>2$, we have $cn+2n-1\leq c2n$. Thereby that indeed $\sum_{i=1}^{2n} \ceil{\log_2(2n/i)}\leq c2n$ holds.
	
	Since both the base case and the inductive step have been performed, by mathematical induction, $\sum_{i=1}^{n} \ceil{\log_2(n/i)}$ is $O(n)$ holds for all $n$ that is a power of $2$. Q.E.D.
	%Since $n$ is a power of $2$, we can assume that $n=2^t$, where $t$ is a positive integer. Thus we have:
	%\begin{equation*}
	%	\sum_{i=1}^{n} \ceil{\log_2(\frac{n}{i})}=\sum_{i=1}^{n} \ceil{\log_2(\frac{2^t}{i})}=\sum_{i=1}^{n}\ceil{\log_2(2^t)-\log_2(i)}=\sum_{i=1}^{n}\ceil{t-\log_2(i)}
	%\end{equation*}
	
	%Since $i$ is a value from $1$ to $n$, we can assert that $0\leq\log_2(i)$. Thus we have:
	%\begin{equation*}
	%	\sum_{i=1}^{n}\ceil{t-\log_2(i)} \leq \sum_{i=1}^{n} \ceil{t} = tn = n\log_2(n)
	%	\end{equation*}
\section{Theory: Big-Oh and Derivatives}
	In the following questions, suppose that $f,g : \mathbb{R} \to \mathbb{R}$ are differentiable and strictly increasing ($f'(x) >0$ and $g'(x)>0$ for all $x$). Prove the statement in each question, or construct a counterexample.
\subsection{Is $f(x)\in O(g(x))$ if and only if $f'(x)\in O(g'(x))$?}
	\emph{Answer:} No, the counterexample is shown as follow:
	
	Suppose we have:
	\begin{equation*}
		f(x) = 3-\frac{1}{e^x}
	\end{equation*}
	\begin{equation*}
		g(x) = 4-\frac{1}{e^{2x}}
	\end{equation*}
	
	Where $f(x)\in O(g(x))$ because:
	\begin{equation*}
		\lim_{x\to\infty} \frac{f(x)}{(g(x))} = \lim_{x\to\infty}\frac{3-\frac{1}{e^x}}{4-\frac{1}{e^{2x}}}=\lim_{x\to\infty} \frac{\frac{3e^x-1}{e^x}}{\frac{4e^{2x}-1}{e^{2x}}}=\lim_{x\to\infty}\frac{e^x(3e^x-1)}{4e^{2x}-1}=\lim_{x\to\infty}\frac{3e^{2x}-e^x}{4e^{2x}-1}
	\end{equation*}
	
	According to L'Hospital's rule,
	\begin{equation*}
		\lim_{x\to\infty}\frac{3e^{2x}-e^x}{4e^{2x}-1}=\lim_{e^x\to\infty} \frac{6e^x-1}{8e^x} = \lim_{e^x\to\infty}\frac{6}{8} = \frac{3}{4} = 0.75
	\end{equation*}
	
	Since $0\leq0.75<\infty$, it is indeed that $f(x)\in O(g(x))$.
	
	However, when we have
	\begin{equation*}
		f'(x) = \frac{1}{e^x}
	\end{equation*}
	\begin{equation*}
		g'(x) = \frac{2}{e^{2x}}
	\end{equation*}
	
	To examine $f'(x)\in O(g'(x))$, we can calculate  $\lim_{x\to\infty}\frac{f'(x)}{g'(x)}$ by:
	\begin{equation*}
		\lim_{x\to\infty}\frac{f'(x)}{g'(x)}= \lim_{x\to\infty}\frac{\frac{1}{e^x}}{\frac{2}{e^{2x}}}=\lim_{x\to\infty}\frac{e^x}{2}=\infty
	\end{equation*}
	
	Such results indicate that $f'(x) \in \omega(g'(x))$, which stands against $f'(x)\in O(g'(x))$. So that the statement in this question is not true.
\subsection{Is it true that if $\lim_{x\to+\infty} f'(x)=0$, then $f(x)\in O(1)$?}
	\emph{Answer:} No, the counterexample is shown as follow:
	
	Suppose we have:
	\begin{equation*}
		f(x) = \ln(x)
	\end{equation*}
	
	Where $f'(x) = \frac{1}{x}$, and $\lim_{x\to+\infty} f'(x)=0$, however, 
	
	\begin{equation*}
		\lim_{x\to+\infty}\frac{\ln(x)}{1} = \lim_{x\to+\infty} \ln(x) = +\infty
	\end{equation*}
	
	So that $f'(x) \in \omega(1)$, which stands against $f'(x)\in O(1)$. So that the statement in this question is not true.
\section{Theory: Properties of Big-Oh Notation}
\subsection{Let $f,g: \mathbb{R} \to \mathbb{R}$ be continuous and strictly increasing. Is it necessarily the case that $f \in O(g(x))$ or $g \in O(f(x))$? Prove, or provide a counterexample.}
	\emph{Answer:} Yes, the statement is true, prove as follows:
	
	According to the limit definition, the statement in the question is equivalent to: $\exists c_1=\lim_{x\to+\infty} \frac{f(x)}{g(x)}$ or $c_2=\lim_{x\to+\infty}\frac{g(x)}{f(x)}$, where $0\leq c_1<+\infty$ or $0\leq c_2<+\infty$
	
	Thus we have:
	\begin{equation*}
		1=\lim_{x\to+\infty} \frac{f(x)}{g(x)} \times \frac{g(x)}{f(x)} = c_1 \times c_2
	\end{equation*}
	
	Since the definition field of $f(x)$ and $g(x)$ is $\mathbb{R}$, we can divide this field into these parts:
	
	First, when $c_1 \in (-\infty,0)$, it is not possible because in this case either $f(x)$ or $g(x)$ is not positive, which go against the condition given in the statement.
	
	Second, when $c_1 = 0$, we can simple conclude that $f(x)\in O(g(x))$ because of the definition and $g(x)\in \omega(f(x))$ because in this case $c_2 = +\infty$.
	
	Third, when $c_1 \in(0,+\infty)$, as is discussed above, we can also assert that $f(x)\in O(g(x))$ because of the definition.
	
	Forth, when $c_2 = 0$, it is like the opposite of the second case that $f(x)\in \omega(g(x))$ and $g(x)\in O(f(x))$
	
	To sum up, the statement is true.
\subsection{R-1.18}
	Show that $f(n)$ is $O(g(n))$ if and only if $g(n)$ is $\Omega(f(n))$.

	\emph{Proof:}

	$\to$: if we have $g(b)\in \Omega(f(n))$, which means $\exists 0<c\leq \infty, \lim_{n\to\infty} \frac{g(n)}{f(n)}=c$. We can assert that $\lim_{n\to\infty}g(n) \not =0$. So when considering $\lim_{n\to\infty}\frac{f(n)}{g(n)} =c'$, it is simply that $c' =\frac{1}{c}$. Since $0<c\leq \infty$, we can conclude that $0\leq c'<0$, which means $f(n)\in O(g(n))$.
	
	$\gets$: if we have $f(n)\in O(g(n))$, which means $\exists 0\leq c< \infty, \lim_{n\to\infty} \frac{f(n)}{g(n)}=c$. We can assert that $\lim_{n\to\infty}f(n) \not =0$. So when considering $\lim_{n\to\infty}\frac{g(n)}{f(n)} =c'$, it is simply that $c' =\frac{1}{c}$. Since $0\leq c< \infty$, we can conclude that $0< c'\leq0$, which means $g(n) \in \Omega(f(n))$.
\subsection{R-1.19}
	Show that if $p(n)$ is a polynomial in $n$, then $\log p(n)$ is $O(\log n)$.

	\emph{Proof:}

	Since $p(n)$ is a polynomial in $n$, which means 
	\begin{equation*}
		\exists a_0,a_1,a_2,a_3,...,a_m \in \mathbb{R}, p(n) = a_mn^m + a_{m-1}n^{m-1}+...+a_3n^3+a_2n^2+a_1n^1+a_0
	\end{equation*}

	Thus we have:
	\begin{equation*}
		\lim_{n\to\infty} \frac{\log(p(n))}{\log(n)}=\lim_{n\to\infty}\frac{\log (\sum_{i=0}^{m}a_in^i)}{\log (n)}=\lim_{n\to\infty}\frac{\ln (\sum_{i=0}^{m}a_in^i)}{\ln (n)}
	\end{equation*}
	
	According to L'Hospital's rule, we have:
	\begin{equation*}
		\lim_{n\to\infty}\frac{\ln (\sum_{i=0}^{m}a_in^i)}{\ln (n)}= \lim_{n\to\infty}\frac{\sum_{i=1}^{m}ia_in^{i-1}\times\frac{1}{\sum_{i=0}^{m}a_in^i}}{\frac{1}{n}}=\lim_{n\to\infty}\frac{\sum_{i=1}^{m}ia_in^{i}}{\sum_{i=0}^{m}a_in^i}
	\end{equation*}
	
	When we apply L'Hospital's rule for $m$ times, we have:
	\begin{equation*}
		\lim_{n\to\infty}\frac{\sum_{i=1}^{m}ia_in^{i}}{\sum_{i=0}^{m}a_in^i}=\lim_{n\to\infty}\frac{m\prod_{i=1}^{m}i}{\prod_{i=1}^{m}i}=m
	\end{equation*}
	
	Since $m$ is the level of $p(n)$, $m\in \mathbb{N_+}$. Thus $0\leq m<+\infty$, so that $\log p(n) \in O(\log n)$.
\section{Application: Matrix Multiplication}
\subsection{Provide a tight asymptotic upper bound on the runtime of easy-mutiply in terms of $n$.}
	\emph{Answer:} To calculate the asymptotic upper bound, we need to rewrite this algorithm into \emph{while} loop:
	\begin{algorithmic}
		\State $Result \gets $an $n \times n$ matrix of zeros \Comment $n\times n$ units of time
		\State $i\gets 1$ \Comment 1 unit of time
		\While{$i\leq n$} \Comment $n+1$ units of time
			\State $j\gets 1$ \Comment $n$ units of time
			\While{$j\leq n$} \Comment $n\times(n+1)$ units of time
				\State $k\gets 1$ \Comment $n\times n$ units of time
				\While{$k\leq n$} \Comment $n\times n\times (n+1)$ units of time
					\State $Result[i][j]\gets A[i][k]*B[k][j]$ \Comment $8\times n^{3}$ units of time
					\State $k\gets k+1$ \Comment $2\times n^3$ units of time
				\EndWhile
				\State $j\gets j+1$ \Comment $2\times n^2$ units of time
			\EndWhile
			\State $i\gets i+1$ \Comment $2\times n$ units of time
		\EndWhile
		\State \Return $Result$\Comment $1$ unit of time
	\end{algorithmic}

	So the total time of execution is $11n^3+5n^2+5n+3$, so that the upper bound should be $\Theta(n^3)$.
\subsection{What is the asymptotic runtime of combined-multiply in terms of $n$?}
	\emph{Answer:} Since when $n\geq100$, this algorithm will use \emph{Strassen Matrix Multiplication} method to calculate the multiplication, we can assert that the asymptotic runtime of combined-multiply is the same as \emph{Strassen Matrix Multiplication}, so it is $O(n^{2.807})$.
\section{``Application'': Spaghetti Sort}
\subsection{Describe the asymptotic runtime of unmodified spaghetti-sort.}
	
\subsection{Suppose that we require that the input list is \emph{not} a list of arbitrary natural numbers, but is instead a list of 64-bit unsigned integers. What is a tight bound on the asymptotic runtime of spaghetti sort now?}
	
\subsection{In the light of the answer to the previous question, why/ why not would we want to use spaghetti-sort for sorting 64-bit unsigned integers in practice?}
	
\section{Algorithm Design: Finding Cycles}
\end{document}